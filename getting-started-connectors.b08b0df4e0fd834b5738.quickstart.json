{"apiVersion":"console.openshift.io/v1","kind":"QuickStarts","metadata":{"name":"connectors-getting-started","annotations":{"draft":false,"order":6}},"spec":{"version":0.1,"type":{"text":"Quick Start","color":"green"},"displayName":"Getting started with Red Hat OpenShift Connectors","durationMinutes":20,"icon":"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzOCIgaGVpZ2h0PSIzOCIgdmlld0JveD0iMCAwIDM4IDM4Ij48ZGVmcz48c3R5bGU+LmF7ZmlsbDojZmZmO30uYntmaWxsOiNlMDA7fTwvc3R5bGU+PC9kZWZzPjxwYXRoIGQ9Ik0yOCwxSDEwYTksOSwwLDAsMC05LDlWMjhhOSw5LDAsMCwwLDksOUgyOGE5LDksMCwwLDAsOS05VjEwYTksOSwwLDAsMC05LTlaIi8+PHBhdGggY2xhc3M9ImEiIGQ9Ik0yMiwyNS42MjVIMTNhLjYyNS42MjUsMCwwLDEsMC0xLjI1aDlhMi4zNzUsMi4zNzUsMCwwLDAsMC00Ljc1SDE1YTMuNjI1LDMuNjI1LDAsMCwxLDAtNy4yNUgyNWEuNjI1LjYyNSwwLDAsMSwwLDEuMjVIMTVhMi4zNzUsMi4zNzUsMCwwLDAsMCw0Ljc1aDdhMy42MjUsMy42MjUsMCwwLDEsMCw3LjI1WiIvPjxwYXRoIGNsYXNzPSJiIiBkPSJNMjUsMTYuNjI1QTMuNjI1LDMuNjI1LDAsMSwxLDI4LjYyNSwxMywzLjYyODg2LDMuNjI4ODYsMCwwLDEsMjUsMTYuNjI1Wm0wLTZBMi4zNzUsMi4zNzUsMCwxLDAsMjcuMzc1LDEzLDIuMzc3NywyLjM3NzcsMCwwLDAsMjUsMTAuNjI1WiIvPjxwYXRoIGNsYXNzPSJiIiBkPSJNMTMsMjguNjI1QTMuNjI1LDMuNjI1LDAsMSwxLDE2LjYyNSwyNSwzLjYyODg2LDMuNjI4ODYsMCwwLDEsMTMsMjguNjI1Wm0wLTZBMi4zNzUsMi4zNzUsMCwxLDAsMTUuMzc1LDI1LDIuMzc3NywyLjM3NzcsMCwwLDAsMTMsMjIuNjI1WiIvPjwvc3ZnPg==","description":"<div id=\"description\" class=\"exampleblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Learn how to configure connections between Red Hat OpenShift Streams for Apache Kafka and third-party systems by using Red Hat OpenShift Connectors.</p>\n</div>\n</div>\n</div>","prerequisites":["Complete the <a href=\"https://console.redhat.com/application-services/learning-resources?quickstart=getting-started\">Getting started with OpenShift Streams for Apache Kafka</a> quick start.","If you plan to use your own OpenShift cluster to deploy your Connectors instances, a cluster administrator must install the Connectors add-on as described in <a href=\"https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01\">Adding the Red Hat OpenShift Connectors add-on to your OpenShift cluster</a>."],"introduction":"<div id=\"introduction\" class=\"exampleblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Welcome to the quick start for Red Hat OpenShift Connectors.</p>\n</div>\n<div class=\"paragraph\">\n<p>In this quick start, you learn how to create a source connector and sink connector and send data to and from OpenShift Streams for Apache Kafka.</p>\n</div>\n<div class=\"paragraph\">\n<p>A <em>source</em> connector allows you to send data from an external system to OpenShift Streams for Apache Kafka.</p>\n</div>\n<div class=\"paragraph\">\n<p>A <em>sink</em> connector allows you to send data from OpenShift Streams for Apache Kafka to an external system.</p>\n</div>\n</div>\n</div>","tasks":["<div class=\"sect1\">\n<h2 id=\"proc-verifying-prerequisites-for-connectors_getting-started-connectors\">Verifying the prerequisites for using Red Hat OpenShift Connectors</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Before you use OpenShift Connectors, you must complete the following prerequisites:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Determine which OpenShift environment to use for your <em>Connectors namespace</em>. The Connectors namespace is where your OpenShift Connectors instances are deployed.</p>\n</li>\n<li>\n<p>Configure Red Hat OpenShift Streams for Apache Kafka for use with OpenShift Connectors.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Determining which OpenShift environment to use for your Connectors namespace</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>You have three choices:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>The hosted preview environment</strong></p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The Connectors instances are hosted on a multitenant OpenShift Dedicated cluster that is owned by Red Hat.</p>\n</li>\n<li>\n<p>You can create four Connectors instances at a time.</p>\n</li>\n<li>\n<p>The preview environment applies 48-hour expiration windows, as described in <a href=\"https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/8190dc9e-249c-4207-bd69-096e5dd5bc64\" target=\"_blank\" rel=\"noopener\">Red Hat OpenShift Connectors Preview guidelines</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><strong>Your own Red Hat OpenShift Dedicated Trial environment</strong></p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You have access to your own Red Hat OpenShift Dedicated Trial environment.</p>\n</li>\n<li>\n<p>You can create an unlimited number of Connectors instances.</p>\n</li>\n<li>\n<p>Your OpenShift Dedicated Trial cluster expires after 60 days.</p>\n</li>\n<li>\n<p>A cluster administrator must install the OpenShift Connectors add-on as described in <a href=\"https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01\" target=\"_blank\" rel=\"noopener\">Adding the Red Hat OpenShift Connectors add-on to your OpenShift cluster</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><strong>Your own OpenShift Service for AWS cluster</strong></p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You have access to your own OpenShift Service for AWS (ROSA) environment.</p>\n</li>\n<li>\n<p>You can create Connectors instances depending on your subscription, as described in <a href=\"https://access.redhat.com/articles/6990631\" target=\"_blank\" rel=\"noopener\">Red Hat OpenShift Connectors Tiers</a>.</p>\n</li>\n<li>\n<p>A cluster administrator must install the OpenShift Connectors add-on as described in <a href=\"https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01\" target=\"_blank\" rel=\"noopener\">Adding the Red Hat OpenShift Connectors add-on to your OpenShift cluster</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Configuring Red Hat OpenShift Streams for Apache Kafka for use with OpenShift Connectors</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>Complete the steps in the <a href=\"https://console.redhat.com/application-services/learning-resources?quickstart=getting-started\">Getting started with Red Hat OpenShift Streams for Apache Kafka</a> quick start to set up the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>A <em>Kafka instance</em> that you can use for OpenShift Connectors. For this example, the Kafka instance is <code>test-connect</code>.</p>\n</li>\n<li>\n<p>A <em>Kafka topic</em> to store messages sent by data sources and make the messages available to data sinks. For this example, the Kafka topic is <code>test-topic</code>.</p>\n</li>\n<li>\n<p>A <em>service account</em> that allows you to connect and authenticate your Connectors instances with your Kafka instance.</p>\n</li>\n<li>\n<p><em>Access rules</em> for the service account that define how your Connectors instances can access and use the topics in your Kafka instance.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Procedure</div>\n<p>Make sure that you have set up the prerequisite components.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Is the Kafka instance listed on the <strong>Kafka Instances</strong> page and is the Kafka instance in the <strong>Ready</strong> state?</p>\n</li>\n<li>\n<p>Is your service account created on the <strong>Service Accounts</strong> page?</p>\n</li>\n<li>\n<p>Did you save your service account credentials to a secure location?</p>\n</li>\n<li>\n<p>Are the permissions for your service account listed on the <strong>Access</strong> page of the Kafka instance?</p>\n</li>\n<li>\n<p>Is the Kafka topic that you created for Connectors listed on the <strong>Topics</strong> page of the Kafka instance?</p>\n</li>\n<li>\n<p>If you plan to use your own OpenShift cluster (OpenShift Dedicated Trial or ROSA) to deploy your OpenShift Connectors instances, has a cluster administrator added the OpenShift Connectors add-on to your Trial cluster?</p>\n</li>\n</ul>\n</div>\n</div>\n</div>","<div class=\"sect1\">\n<h2 id=\"proc-creating-source-connector_getting-started-connectors\">Creating a Connectors instance for a data source</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>A <em>source</em> connector consumes events from an external data source and produces Kafka messages.</p>\n</div>\n<div class=\"paragraph\">\n<p>You configure your Connectors instance to listen for events from the data source and produce a Kafka message for each event. Your Connectors instance sends the messages at regular intervals to the Kafka topic that you created for Connectors.</p>\n</div>\n<div class=\"paragraph\">\n<p>For this example, you create an instance of the Data Generator source connector. The Data Generator is provided for development and testing purposes. You specify the text for a message and how often to send the message.</p>\n</div>\n<div class=\"ulist\"><div class=\"title\">Prerequisites</div><div class=\"task-pflist\"><h6 data-ouia-component-type=\"PF4/Title\" data-ouia-safe=\"true\" data-ouia-component-id=\"OUIA-Generated-Title-1\" class=\"pf-c-title pf-m-md task-pflist-title\">Prerequisites</h6><p class=\"task-pflist-subtitle\">In addtion to the prerequisites for this Quick Start, this step requires:</p><ul class=\"pf-c-list task-pflist-list task-pflist-list--prereq\"><li class=\"task-pflist-list__item--prereq task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>If you want to use a dead letter queue (DLQ) to handle any messaging errors, create a Kafka topic for the DLQ.</p> </span></li></ul></div></div>\n<div class=\"olist\"><div class=\"title\">Procedure</div><div class=\"task-pflist\"><h6 data-ouia-component-type=\"PF4/Title\" data-ouia-safe=\"true\" data-ouia-component-id=\"OUIA-Generated-Title-2\" class=\"pf-c-title pf-m-md task-pflist-title\">Procedure</h6><ol type=\"1\" class=\"pf-c-list task-pflist-list task-pflist-list--proc\"><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>In the Red Hat OpenShift Connectors web console, click <strong>Create a Connectors instance</strong>.</p> </span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Select the connector that you want to use for connecting to a data source.</p> <div class=\"paragraph\">\n<p>You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, to find the Data Generator source connector, type <code>data</code> in the search box. The list is filtered to show only the <strong>Data Generator source</strong> card.</p>\n</div>\n<div class=\"paragraph\">\n<p>Click the card to select the connector, and then click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>On the <strong>Kafka Instance</strong> page, click the card for the OpenShift Streams for Apache Kafka instance that you configured for Connectors. For example, click the <strong>test-connect</strong> card.</p> <div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>On the <strong>Deployment</strong> page, the namespace that you select depends on your OpenShift environment.</p> <div class=\"paragraph\">\n<p>If you&#8217;re using your own OpenShift environment, select the card for the namespace that was created when a cluster administrator added the Connectors service to your cluster, as described in <a href=\"https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01\" target=\"_blank\" rel=\"noopener\">Adding the Red Hat OpenShift Connectors add-on to your OpenShift cluster</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you&#8217;re using the hosted preview environment, click <strong>Create preview namespace</strong> to provision a namespace for hosting the Connectors instances that you create.</p>\n</div>\n<div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Specify the core configuration for your Connectors instance:</p> <div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Type a name for your Connectors instance. For example, type <code>hello world generator</code>.</p>\n</li>\n<li>\n<p>In the <strong>Client ID</strong> and <strong>Client Secret</strong> fields, type the credentials for the service account that you created for Connectors and then click <strong>Next</strong>.</p>\n</li>\n</ol>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Provide connector-specific configuration. For the Data Generator, provide the following information:</p> <div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p><strong>Topic Name</strong>: Type the name of the Kafka topic that you created for Connectors. For example, type <code>test-topic</code>.</p>\n</li>\n<li>\n<p><strong>Content Type</strong>: Accept the default, <code>text/plain</code>.</p>\n</li>\n<li>\n<p><strong>Message</strong>: Type the content of the message that you want the Connectors instance to send to the Kafka topic. For example, type <code>Hello World!!</code>.</p>\n</li>\n<li>\n<p><strong>Period</strong>: Specify the interval (in milliseconds) at which you want the Connectors instance to send messages to the Kafka topic. For example, to send a message every 10 seconds, specify <code>10000</code>.</p>\n</li>\n<li>\n<p><strong>Data Shape Produces Format</strong>: Accept the default, <code>application/octet-stream</code>.</p>\n<div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div>\n</li>\n</ol>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Select one of the following error handling policies for your Connectors instance:</p> <div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Stop</strong>: If a message fails to send, the Connectors instance stops running and changes its status to the <strong>Failed</strong> state. You can view the error message.</p>\n</li>\n<li>\n<p><strong>Ignore</strong>: If a message fails to send, the Connectors instance ignores the error and continues to run. No error message is logged.</p>\n</li>\n<li>\n<p><strong>Dead letter queue</strong>: If a message fails to send, the Connectors instance sends error details to the Kafka topic that you created for the DLQ.</p>\n<div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div>\n</li>\n</ul>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Review the summary of the configuration properties and then click <strong>Create Connectors instance</strong>.</p> <div class=\"paragraph\">\n<p>Your Connectors instance is listed on the <strong>Connectors Instances</strong> page. After a couple of seconds, the status of your Connectors instance changes to the <strong>Ready</strong> state and it starts producing messages and sending them to its associated Kafka topic.</p>\n</div>\n<div class=\"paragraph\">\n<p>From the <strong>Connectors Instances</strong> page, you can stop, start, duplicate, and delete your Connectors instance, as well as edit its configuration, by clicking the options icon (three vertical dots).</p>\n</div></span></li></ol></div></div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Does your source Connectors instance generate messages?</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the Red Hat OpenShift Application Services web console, select <strong>Streams for Apache Kafka</strong> &gt; <strong>Kafka Instances</strong>.</p>\n</li>\n<li>\n<p>Click the Kafka instance that you created for connectors. For example, click <strong>test-connect</strong>.</p>\n</li>\n<li>\n<p>Click the <strong>Topics</strong> tab and then click the topic that you specified for your source Connectors instance. For example, click <strong>test-topic</strong>.</p>\n</li>\n<li>\n<p>Click the <strong>Messages</strong> tab to see a list of <code>Hello World!!</code> messages.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>","<div class=\"sect1\">\n<h2 id=\"proc-creating-sink-connector_getting-started-connectors\">Creating a Connectors instance for a data sink</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>A <em>sink</em> connector consumes messages from a Kafka topic and sends them to an external system.</p>\n</div>\n<div class=\"paragraph\">\n<p>For this example, you use the <strong>HTTP Sink</strong> connector which consumes the Kafka messages (produced by your Data Generator source Connectors instance) and sends the messages to an HTTP endpoint.</p>\n</div>\n<div class=\"ulist\"><div class=\"title\">Prerequisites</div><div class=\"task-pflist\"><h6 data-ouia-component-type=\"PF4/Title\" data-ouia-safe=\"true\" data-ouia-component-id=\"OUIA-Generated-Title-3\" class=\"pf-c-title pf-m-md task-pflist-title\">Prerequisites</h6><p class=\"task-pflist-subtitle\">In addtion to the prerequisites for this Quick Start, this step requires:</p><ul class=\"pf-c-list task-pflist-list task-pflist-list--prereq\"><li class=\"task-pflist-list__item--prereq task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>You created a Data Generator source Connectors instance.</p> </span></li><li class=\"task-pflist-list__item--prereq task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>For the data sink example, open the free <a href=\"https://webhook.site\" target=\"_blank\" rel=\"noopener\">webhook.site</a> in a browser window. The <code>webhook.site</code> page provides a unique URL that you copy for use as an HTTP data sink.</p> </span></li><li class=\"task-pflist-list__item--prereq task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>If you want to use a dead letter queue (DLQ) to handle any messaging errors, create a Kafka topic for the DLQ.</p> </span></li></ul></div></div>\n<div class=\"olist\"><div class=\"title\">Procedure</div><div class=\"task-pflist\"><h6 data-ouia-component-type=\"PF4/Title\" data-ouia-safe=\"true\" data-ouia-component-id=\"OUIA-Generated-Title-4\" class=\"pf-c-title pf-m-md task-pflist-title\">Procedure</h6><ol type=\"1\" class=\"pf-c-list task-pflist-list task-pflist-list--proc\"><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>In the Red Hat OpenShift Connectors web console, click <strong>Create Connectors instance</strong>.</p> </span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Select the sink connector that you want to use:</p> <div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>For example, type <code>http</code> in the search field. The list of Connectors is filtered to show the <strong>HTTP sink</strong> connector.</p>\n</li>\n<li>\n<p>Click the <strong>HTTP sink</strong> card and then click <strong>Next</strong>.</p>\n</li>\n</ol>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>On the <strong>Kafka Instance</strong> page, select the OpenShift Streams for Apache Kafka instance for the connector to work with. For example, select <strong>test-connect</strong>.</p> <div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>On the <strong>Deployment</strong> page, the namespace that you select depends on your OpenShift environment.</p> <div class=\"paragraph\">\n<p>If you&#8217;re using your own OpenShift environment, select the card for the namespace that was created when a cluster administrator added the Connectors service to your cluster.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you&#8217;re using the hosted preview environment, click the <strong>preview namespace</strong> that you provisioned when you created the source connector.</p>\n</div>\n<div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Provide the core configuration for your connector:</p> <div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Type a unique name for the connector. For example, type <code>hello world receiver</code>.</p>\n</li>\n<li>\n<p>In the <strong>Client ID</strong> and <strong>Client Secret</strong> fields, type the credentials for the service account that you created for Connectors and then click <strong>Next</strong>.</p>\n</li>\n</ol>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Provide the connector-specific configuration for your HTTP sink Connectors instance:</p> <div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p><strong>Topic Names</strong>: Type the name of the topic that you used for the source Connectors instance. For example, type <code>test-topic</code>.</p>\n</li>\n<li>\n<p><strong>Method</strong>: Accept the default, <code>POST</code>.</p>\n</li>\n<li>\n<p><strong>URL</strong>: Type your unique URL from the <a href=\"https://webhook.site\" target=\"_blank\" rel=\"noopener\">webhook.site</a>.</p>\n</li>\n<li>\n<p><strong>Data Shape Consumes Format</strong>: Accept the default, <code>application/octet-stream</code>.</p>\n<div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div>\n</li>\n</ol>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Select an error handling policy for your Connectors instance. For example, select <strong>Stop</strong>.</p> <div class=\"paragraph\">\n<p>Click <strong>Next</strong>.</p>\n</div></span></li><li class=\"task-pflist-list__item--proc task-pflist-list__item\"><span class=\"task-pflist-list__item__content\"><p>Review the summary of the configuration properties and then click <strong>Create Connectors instance</strong>.</p> <div class=\"paragraph\">\n<p>Your Connectors instance is added to the <strong>Connectors Instances</strong> page.</p>\n</div>\n<div class=\"paragraph\">\n<p>After a couple of seconds, the status of your Connectors instance changes to the <strong>Ready</strong> state. It consumes messages from the associated Kafka topic and sends them to the data sink (for this example, the data sink is the HTTP URL that you provided).</p>\n</div></span></li></ol></div></div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Open a web browser tab to your custom URL for the <a href=\"https://webhook.site\" target=\"_blank\" rel=\"noopener\">webhook.site</a>. Do you see HTTP POST calls with <code>\"Hello World!!\"</code> messages?</p>\n</li>\n</ul>\n</div>\n<div id=\"conclusion\" class=\"exampleblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Congratulations! You successfully completed the Red Hat OpenShift Connectors Getting Started quick start.</p>\n</div>\n</div>\n</div>\n</div>\n</div>"],"conclusion":"<div id=\"conclusion\" class=\"exampleblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Congratulations! You successfully completed the Red Hat OpenShift Connectors Getting Started quick start.</p>\n</div>\n</div>\n</div>"}}